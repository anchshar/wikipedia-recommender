## To kill all containers
docker kill $(docker ps -q)


## To start spark, go into the driectory and then -

docker-compose up -d 

## To start Hbase, got to the directory
docker-compose -f docker-compose-distributed-local.yml up -d


## To go into a container
docker exec -it <container_name> bash

## Get hbase into a usable state, go inside hbase-master container and then
hbase-daemon.sh restart thrift

## Create table in hbase shell
create 'articles', 'user_cf', 'article_cf', 'cf1'

## start spark on local - Download and then:
./spark-2.4.4-bin-hadoop2.7/sbin/start-master.sh
./spark-2.4.4-bin-hadoop2.7/sbin/start-slave.sh 0.0.0.0:7077


## Submit spark job on local machine
 ./spark-2.4.4-bin-hadoop2.7/bin/spark-submit --driver-memory 5g --executor-memory 6g --master spark://0.0.0.0:7077 /Users/anchit/Google\ Drive\ File\ Stream/My\ Drive/Courses/Fall\ 2019/Big\ Data/wikipedia-recommender/main.py


## Submit spark job(Docker/obsolete)
./spark/bin/spark-submit --master spark://0.0.0.0:7077 wikipedia-recommender/main.py
./spark/bin/spark-submit --master  spark://0.0.0.0:7077 --deploy-mode client wikipedia-recommender/main.py

## copy data from machine to container
docker cp -a  wikipedia-recommender <spark_container>:/




